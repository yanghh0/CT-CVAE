python ../parlai/scripts/train_model.py \
    --task dailydialog,wizard_of_wikipedia,empathetic_dialogues,convai2 \
    --model seq2seq \
    --init-model ../../checkpoint/seq2seq/pretrain/model \
    --model-file ../../checkpoint/seq2seq/joint-fine-tuning/model \
    --batchsize 16 \
    --validation-metric loss \
    --skip-generation True \
    --validation-every-n-epochs 0.25 \
    --optimizer adamax \
    --learningrate 1e-4 \
    --dict-tokenizer bpe \
    --dict-lower True \
    --rnn-class lstm \
    --embedding-type glove \
    --embeddingsize 300 \
    --hiddensize 512 \
    --numlayers 4 \
    --bidirectional True \
    --attention dot \
    --attention-time post \
    --lookuptable all \
    --text-truncate 360 \
    --label-truncate 72 \
    --person-tokens True \
    --validation-patience 10
